from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep
from bs4 import BeautifulSoup
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

# å®‰è£ä¸¦å•Ÿå‹• ChromeDriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# ä½¿ç”¨ Selenium å…§å»ºçš„æ–¹å¼ä¾†è‡ªå‹•åµæ¸¬ ChromeDriver
driver = webdriver.Chrome()

# é–‹å•Ÿ Threads ç¶²ç«™
driver.get('https://www.threads.net/?hl=zh-tw')           
sleep(3)

# æ¨¡æ“¬æ»¾å‹•é é¢ï¼Œä½¿æ›´å¤šå…§å®¹è¼‰å…¥
for i in range(5):
    n = f'window.scrollTo(0, {200 + i * 2000})'
    driver.execute_script(n)
    sleep(1)

# å–å¾— HTML å…§å®¹ä¸¦è§£æ
html_content = driver.page_source
soup = BeautifulSoup(html_content, "html.parser")

# æ‰¾å‡ºç‰¹å®šçš„ HTML å…ƒç´ 
target_element = soup.select_one(".x78zum5.xdt5ytf.x1iyjqo2.x1n2onr6")

# é€™å€‹ class å¯èƒ½éœ€è¦ä¾æ“š Threads ç¶²ç«™çš„è®Šæ›´ä¾†èª¿æ•´
# æå–ç‰¹å®š class çš„å…§å®¹
try:
    target_element = soup.select_one(".x78zum5.xdt5ytf.x1iyjqo2.x1n2onr6")
    if target_element:
        classes = target_element.find_all(class_=[
            "x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7",
            "xu9jpxn x1n2onr6 xqcsobp x12w9bfk x1wsgiic xuxw1ft x1bl4301"
        ])
    else:
        classes = []  # å¦‚æœæ‰¾ä¸åˆ°ç›®æ¨™å…ƒç´ ï¼Œå›å‚³ç©ºåˆ—è¡¨
except Exception as e:
    print("ç™¼ç”ŸéŒ¯èª¤ï¼Œç„¡æ³•è§£æ HTMLï¼š", e)
    classes = []

# å„²å­˜çˆ¬å–åˆ°çš„æ–‡ç« å…§å®¹
post = list()
for i in classes:
    post.append(i.get_text(strip=True))

# å°å‡ºæ‰€æœ‰çˆ¬å–åˆ°çš„æ–‡ç« 
print("\nğŸ“Œ çˆ¬å–åˆ°çš„ Threads è²¼æ–‡å¦‚ä¸‹ï¼š")
for i, content in enumerate(post):
    print(f"No. {i+1}: {content}")

input("âœ… æŒ‰ä¸‹ Enter éµé–‹å§‹åš TF-IDF åˆ†æ...")
# ---------------- TF-IDF åˆ†æ ---------------- #

# jieba æ–·è© + ç©ºæ ¼åˆ†éš”
segmented_posts = [" ".join(jieba.cut(p)) for p in post]

# å»ºç«‹ TF-IDF å‘é‡å™¨ä¸¦è½‰æ›æ•´é«”æ–‡ç« 
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(segmented_posts)
feature_names = vectorizer.get_feature_names_out()

# å„²å­˜æ¯ç¯‡æ–‡ç« çš„å‰ 5 å€‹é—œéµè©ï¼ˆåªæœ‰è©ï¼Œä¸è¦åˆ†æ•¸ï¼‰
corpus_tfidf = []

# é¡¯ç¤ºæ¯ç¯‡æ–‡ç« çš„ top é—œéµè©
print("\nğŸ” TF-IDF é—œéµè©åˆ†æï¼š")
for idx in range(len(post)):
    tfidf_scores = tfidf_matrix[idx].toarray()[0]
    word_scores = list(zip(feature_names, tfidf_scores))
    top_keywords = sorted(word_scores, key=lambda x: x[1], reverse=True)[:5]
    keywords_only = [word for word, score in top_keywords if score > 0]
    corpus_tfidf.append(keywords_only)

    print(f"\nğŸ“„ æ–‡ç«  {idx+1}ï¼š{post[idx]}")
    print("â­ å„ªå…ˆé—œéµè©ï¼š")
    for word, score in top_keywords:
        if score == 0:
            continue
        print(f"  {word}: {score:.4f}")

# å„²å­˜æˆ pkl
with open("corpus_tfidf.pkl", "wb") as f:
    pickle.dump(corpus_tfidf, f)

# é—œé–‰ç€è¦½å™¨
input("\nâœ… å·²å„²å­˜ TF-IDF é—œéµè©è‡³ corpus_tfidf.pkl")
driver.quit()